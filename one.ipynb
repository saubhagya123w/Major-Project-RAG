{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55c9cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. API_TOKEN set: False\n",
      "OPENAI available: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Imports and global config\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "# For scraping & parsing\n",
    "import trafilatura\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# For embeddings & vectors\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# For model fallback generation if OpenAI isn't available\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# For API server (when we write app.py)\n",
    "from fastapi import FastAPI, HTTPException, Request, Depends, Header\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "# For reading .env (optional)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Constants / file paths\n",
    "DATA_DIR = Path(\"rag_data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "FAISS_INDEX_PATH = DATA_DIR / \"faiss_index.index\"\n",
    "METADATA_PATH = DATA_DIR / \"metadata.pkl\"\n",
    "INDEXED_URLS_PATH = DATA_DIR / \"indexed_urls.json\"\n",
    "\n",
    "# Environment-configurable secrets\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\", \"CHANGE_ME_API_TOKEN\")  # set this in your env for real usage\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # optional, for best LLM answers\n",
    "\n",
    "# Embedding model name (sentence-transformers)\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"  # small and fast (dim=384)\n",
    "\n",
    "# RAG hyperparams\n",
    "CHUNK_SIZE_WORDS = 400  # approximate words per chunk\n",
    "CHUNK_OVERLAP = 50      # overlap in words\n",
    "TOP_K = 5               # number of docs to retrieve\n",
    "\n",
    "print(\"Configuration loaded. API_TOKEN set:\", API_TOKEN != \"CHANGE_ME_API_TOKEN\")\n",
    "print(\"OPENAI available:\", bool(OPENAI_API_KEY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80c45bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saubh\\AppData\\Local\\Programs\\Python\\Python310\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b989b4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 — fetch_text_from_url(url) using trafilatura with BeautifulSoup fallback\n",
    "def fetch_text_from_url(url: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Fetch clean text from a URL.\n",
    "    Returns dict: {\"url\": url, \"title\": title, \"text\": text}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        downloaded = trafilatura.fetch_url(url)\n",
    "        if downloaded:\n",
    "            text = trafilatura.extract(downloaded, include_comments=False, include_tables=False)\n",
    "            # try to get title\n",
    "            soup = BeautifulSoup(downloaded, \"html.parser\")\n",
    "            title = soup.title.string.strip() if soup.title and soup.title.string else url\n",
    "            if text and len(text) > 50:\n",
    "                return {\"url\": url, \"title\": title, \"text\": text}\n",
    "    except Exception as e:\n",
    "        # fallback to requests + BeautifulSoup\n",
    "        print(f\"trafilatura failed for {url}: {e}\")\n",
    "\n",
    "    # Fallback\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=15, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        # Remove scripts/styles\n",
    "        for s in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "            s.extract()\n",
    "        # heuristics: join text blocks\n",
    "        paragraphs = [p.get_text(separator=\" \", strip=True) for p in soup.find_all([\"p\", \"article\", \"div\"])]\n",
    "        text = \"\\n\".join([p for p in paragraphs if len(p) > 40])\n",
    "        title = soup.title.string.strip() if soup.title and soup.title.string else url\n",
    "        if text.strip() == \"\":\n",
    "            raise ValueError(\"No text extracted from page\")\n",
    "        return {\"url\": url, \"title\": title, \"text\": text}\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not fetch or parse {url}: {e}\")\n",
    "\n",
    "# Quick test (optional)\n",
    "# print(fetch_text_from_url(\"https://huyenchip.com/2024/07/25/genai-platform.html\")[\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20674447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — chunk_text helper\n",
    "import re\n",
    "\n",
    "def chunk_text(text: str, chunk_size_words: int = CHUNK_SIZE_WORDS, overlap: int = CHUNK_OVERLAP):\n",
    "    \"\"\"\n",
    "    Splits text into chunks approximated by words with overlap.\n",
    "    Returns list of dicts: {\"chunk_id\": str, \"text\": str, \"start\": int, \"end\": int}\n",
    "    \"\"\"\n",
    "    words = re.split(r\"\\s+\", text.strip())\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    chunk_idx = 0\n",
    "    while i < len(words):\n",
    "        start = max(0, i - overlap) if chunk_idx > 0 else i\n",
    "        end = min(len(words), i + chunk_size_words)\n",
    "        chunk_words = words[start:end]\n",
    "        chunk_text = \" \".join(chunk_words).strip()\n",
    "        if chunk_text:\n",
    "            chunks.append({\"chunk_id\": f\"chunk_{chunk_idx}\", \"text\": chunk_text, \"start\": start, \"end\": end})\n",
    "        i += chunk_size_words - overlap\n",
    "        chunk_idx += 1\n",
    "    return chunks\n",
    "\n",
    "# Quick test\n",
    "# sample = \" \".join([f\"word{i}\" for i in range(1000)])\n",
    "# print(len(chunk_text(sample)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84b2a882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9f568172934e93bf2c496884f6c08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saubh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\saubh\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a2c1138fcf4b63a56273aa48c1a086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f7bc3977824ea4a2975070cc260629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb7b6d0295c47dbbeaee18fb04f82cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70eb9ee529244a59fa205c24580e72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "117c5de620794f59be7dcb85f686a0b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e813ffc03049f68d38f2bcdb8eb304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495357d8328c40b7a1cb19e07e803890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fca557daf094ed0b114fd9ffc4c1173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff7c801ff78401681cb96dff6cfddbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd6706f1e8046a39a48cd5142c3a62f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new FAISS index and metadata...\n",
      "Vector store ready. Current vectors: 0\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — VectorStoreFAISS class (embedding + faiss)\n",
    "class VectorStoreFAISS:\n",
    "    def __init__(self, embedding_model_name=EMBEDDING_MODEL_NAME, index_path=FAISS_INDEX_PATH, metadata_path=METADATA_PATH):\n",
    "        self.embedding_model_name = embedding_model_name\n",
    "        self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
    "        self.dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        self.index_path = Path(index_path)\n",
    "        self.metadata_path = Path(metadata_path)\n",
    "        self._init_index_and_metadata()\n",
    "\n",
    "    def _init_index_and_metadata(self):\n",
    "        # If index exists, load; else create new index\n",
    "        if self.index_path.exists() and self.metadata_path.exists():\n",
    "            print(\"Loading existing FAISS index and metadata...\")\n",
    "            self.index = faiss.read_index(str(self.index_path))\n",
    "            with open(self.metadata_path, \"rb\") as f:\n",
    "                self.metadata = pickle.load(f)\n",
    "        else:\n",
    "            print(\"Creating new FAISS index and metadata...\")\n",
    "            # We'll use Inner Product on normalized vectors for cosine similarity\n",
    "            self.index = faiss.IndexFlatIP(self.dim)\n",
    "            self.metadata = {\"documents\": [], \"next_id\": 0}\n",
    "            self.save()\n",
    "\n",
    "    def save(self):\n",
    "        faiss.write_index(self.index, str(self.index_path))\n",
    "        with open(self.metadata_path, \"wb\") as f:\n",
    "            pickle.dump(self.metadata, f)\n",
    "\n",
    "    def _embed_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        embs = self.embedding_model.encode(texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "        # Normalize for cosine similarity with inner product\n",
    "        faiss.normalize_L2(embs)\n",
    "        return embs\n",
    "\n",
    "    def add_documents(self, docs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        docs: list of dicts with keys: 'text', 'source', 'source_title', 'chunk_id'\n",
    "        \"\"\"\n",
    "        texts = [d[\"text\"] for d in docs]\n",
    "        embs = self._embed_texts(texts)\n",
    "        n_before = self.index.ntotal\n",
    "        self.index.add(embs)\n",
    "        # record metadata per vector (order matters)\n",
    "        for i, d in enumerate(docs):\n",
    "            entry = {\n",
    "                \"id\": self.metadata[\"next_id\"],\n",
    "                \"text\": d[\"text\"],\n",
    "                \"source\": d.get(\"source\"),\n",
    "                \"source_title\": d.get(\"source_title\"),\n",
    "                \"chunk_id\": d.get(\"chunk_id\"),\n",
    "            }\n",
    "            self.metadata[\"documents\"].append(entry)\n",
    "            self.metadata[\"next_id\"] += 1\n",
    "        self.save()\n",
    "        return {\"added\": len(docs), \"total\": self.index.ntotal, \"was\": n_before}\n",
    "\n",
    "    def search(self, query: str, top_k: int = TOP_K):\n",
    "        q_emb = self._embed_texts([query])\n",
    "        D, I = self.index.search(q_emb, top_k)\n",
    "        results = []\n",
    "        for score, idx in zip(D[0], I[0]):\n",
    "            if idx < 0 or idx >= len(self.metadata[\"documents\"]):\n",
    "                continue\n",
    "            md = self.metadata[\"documents\"][idx]\n",
    "            results.append({\"score\": float(score), \"id\": md[\"id\"], \"text\": md[\"text\"], \"source\": md[\"source\"], \"source_title\": md.get(\"source_title\")})\n",
    "        return results\n",
    "\n",
    "# Create vector store (instantiates model, may take a moment)\n",
    "vector_store = VectorStoreFAISS()\n",
    "print(\"Vector store ready. Current vectors:\", vector_store.index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d65cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 — Indexing functions\n",
    "def load_indexed_urls():\n",
    "    if INDEXED_URLS_PATH.exists():\n",
    "        with open(INDEXED_URLS_PATH, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return []\n",
    "\n",
    "def save_indexed_urls(urls):\n",
    "    with open(INDEXED_URLS_PATH, \"w\") as f:\n",
    "        json.dump(urls, f, indent=2)\n",
    "\n",
    "def index_url(url: str, force: bool = False) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Fetch page, chunk, and add to vector store. Returns dict with status.\n",
    "    \"\"\"\n",
    "    indexed = load_indexed_urls()\n",
    "    if (url in indexed) and (not force):\n",
    "        return {\"status\": \"skipped\", \"reason\": \"already indexed\", \"url\": url}\n",
    "    try:\n",
    "        page = fetch_text_from_url(url)\n",
    "        chunks = chunk_text(page[\"text\"])\n",
    "        docs = []\n",
    "        for c in chunks:\n",
    "            docs.append({\n",
    "                \"text\": c[\"text\"],\n",
    "                \"source\": url,\n",
    "                \"source_title\": page.get(\"title\", url),\n",
    "                \"chunk_id\": c[\"chunk_id\"],\n",
    "            })\n",
    "        res = vector_store.add_documents(docs)\n",
    "        # update indexed urls\n",
    "        indexed.append(url)\n",
    "        save_indexed_urls(indexed)\n",
    "        return {\"status\": \"success\", \"url\": url, \"added_chunks\": res[\"added\"]}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"failed\", \"url\": url, \"error\": str(e)}\n",
    "\n",
    "def index_urls(urls: List[str], force=False):\n",
    "    results = {\"indexed_url\": [], \"failed_url\": []}\n",
    "    for u in urls:\n",
    "        r = index_url(u, force=force)\n",
    "        if r[\"status\"] == \"success\":\n",
    "            results[\"indexed_url\"].append(u)\n",
    "        else:\n",
    "            results[\"failed_url\"].append({\"url\": u, \"reason\": r.get(\"reason\", r.get(\"error\"))})\n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "# print(index_url(\"https://huyenchip.com/2024/07/25/genai-platform.html\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da9b9128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 — RAG answer generation with citation\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "# Optional: OpenAI client if API key available\n",
    "try:\n",
    "    import openai\n",
    "    if OPENAI_API_KEY:\n",
    "        openai.api_key = OPENAI_API_KEY\n",
    "except Exception:\n",
    "    openai = None\n",
    "\n",
    "# Prepare HF fallback model (lazy init)\n",
    "HF_MODEL = None\n",
    "HF_TOKENIZER = None\n",
    "def init_hf_model():\n",
    "    global HF_MODEL, HF_TOKENIZER\n",
    "    if HF_MODEL is None:\n",
    "        model_name = \"google/flan-t5-base\"  # balanced model\n",
    "        print(\"Loading HF model (this may take time)... Model:\", model_name)\n",
    "        HF_TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n",
    "        HF_MODEL = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return HF_MODEL, HF_TOKENIZER\n",
    "\n",
    "def build_prompt_with_context(question: str, retrieved: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Create a prompt instructing model to use only the provided context chunks.\n",
    "    Include citation markers [1], [2] mapping to source URLs.\n",
    "    \"\"\"\n",
    "    # unique sources and mapping\n",
    "    sources = []\n",
    "    for r in retrieved:\n",
    "        if r[\"source\"] not in sources:\n",
    "            sources.append(r[\"source\"])\n",
    "    # Build context\n",
    "    context_parts = []\n",
    "    for i, r in enumerate(retrieved, start=1):\n",
    "        src_index = sources.index(r[\"source\"]) + 1\n",
    "        header = f\"[{src_index}] {r.get('source_title', r['source'])} ({r['source']})\"\n",
    "        part = f\"{header}\\n{r['text']}\"\n",
    "        context_parts.append(part)\n",
    "    context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
    "    # System + user prompt\n",
    "    prompt = textwrap.dedent(f\"\"\"\n",
    "    You are an assistant that must answer the question using ONLY the provided CONTEXT sections. \n",
    "    Use the context to answer and do NOT hallucinate extra facts.\n",
    "    For each factual statement that comes from a context, include an inline citation like [1], [2] referencing the source list below.\n",
    "    If the context doesn't contain the answer, say \"I don't know based on the provided sources.\" and suggest where to search.\n",
    "\n",
    "    CONTEXT:\n",
    "    {context}\n",
    "\n",
    "    QUESTION:\n",
    "    {question}\n",
    "\n",
    "    ANSWER (provide concise answer; include citations inline and at the end list unique source URLs):\n",
    "    \"\"\").strip()\n",
    "    return prompt, sources\n",
    "\n",
    "def call_llm(prompt: str, max_tokens: int = 512) -> str:\n",
    "    # Prefer OpenAI chat if available\n",
    "    if openai and OPENAI_API_KEY:\n",
    "        try:\n",
    "            resp = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                          {\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=max_tokens,\n",
    "                temperature=0.0,\n",
    "            )\n",
    "            return resp.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI call failed:\", e)\n",
    "\n",
    "    # Fallback to HF model\n",
    "    try:\n",
    "        model, tokenizer = init_hf_model()\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids\n",
    "        outputs = model.generate(input_ids, max_new_tokens=256, do_sample=False)\n",
    "        out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return out.strip()\n",
    "    except Exception as e:\n",
    "        print(\"HF generation failed:\", e)\n",
    "        # final fallback: return concatenated contexts + a note\n",
    "        return \" / \".join([c['text'][:300] for c in retrieved]) + \"\\n\\n[Note: fallback generation used; set OPENAI_API_KEY for better answers]\"\n",
    "\n",
    "def generate_answer(messages: List[Dict[str, str]], top_k: int = TOP_K):\n",
    "    \"\"\"\n",
    "    messages: list of {\"role\": \"user\"/\"assistant\", \"content\": \"...\"}\n",
    "    Returns: {\"answer\": \"...\", \"citations\": [\"url1\",\"url2\"]}\n",
    "    \"\"\"\n",
    "    # get last user message content\n",
    "    last_user = None\n",
    "    for m in reversed(messages):\n",
    "        if m.get(\"role\") == \"user\":\n",
    "            last_user = m.get(\"content\")\n",
    "            break\n",
    "    if not last_user:\n",
    "        raise ValueError(\"No user message found in messages\")\n",
    "\n",
    "    # Retrieve similar chunks\n",
    "    retrieved = vector_store.search(last_user, top_k=top_k)\n",
    "    if not retrieved:\n",
    "        return {\"answer\": \"No indexed content found. Please index some URLs first.\", \"citations\": []}\n",
    "\n",
    "    # Build prompt and call LLM\n",
    "    prompt, sources = build_prompt_with_context(last_user, retrieved)\n",
    "    answer_text = call_llm(prompt)\n",
    "    # Prepare citations (unique)\n",
    "    citations = sources\n",
    "    return {\"answer\": answer_text, \"citations\": citations}\n",
    "\n",
    "# Quick test (after indexing pages): \n",
    "# messages = [{\"role\":\"user\",\"content\":\"What is a GenAI platform?\"}]\n",
    "# print(generate_answer(messages))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2b91742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote app.py to current directory. Run it with:\n",
      "uvicorn app:app --reload --port 8000\n",
      "Remember to set environment variable API_TOKEN to match X-API-KEY used by the client.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Write full FastAPI app to app.py\n",
    "APP_CODE = r'''\n",
    "from fastapi import FastAPI, HTTPException, Request, Header\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import os, json\n",
    "\n",
    "# import local helpers (we assume running from same folder)\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# We'll import functions/classes defined in the notebook environment by reloading\n",
    "# For the simple local setup, we also provide a thin wrapper that reuses the\n",
    "# vector_store and functions if the notebook started the objects; else we lazy import.\n",
    "\n",
    "app = FastAPI(title=\"RAG Website Retrieval API\")\n",
    "\n",
    "# Auth dependency\n",
    "API_TOKEN = os.getenv(\"API_TOKEN\", \"CHANGE_ME_API_TOKEN\")\n",
    "def check_api_key(x_api_key: Optional[str] = Header(None)):\n",
    "    if x_api_key != API_TOKEN:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid or missing X-API-KEY header\")\n",
    "\n",
    "class IndexRequest(BaseModel):\n",
    "    url: List[str]\n",
    "\n",
    "class ChatMessage(BaseModel):\n",
    "    content: str\n",
    "    role: str\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    messages: List[ChatMessage]\n",
    "\n",
    "@app.post(\"/api/v1/index\")\n",
    "async def index_endpoint(body: IndexRequest, x_api_key: str = Header(None)):\n",
    "    check_api_key(x_api_key)\n",
    "    # We'll call functions from the notebook environment if available (for local dev)\n",
    "    # Try to import helper functions from rag notebook module or rely on index_urls being in globals\n",
    "    try:\n",
    "        from __main__ import index_urls\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=500, detail=\"Indexer functions not found. Start server from notebook or ensure helper functions are accessible.\")\n",
    "    # index the urls\n",
    "    results = index_urls(body.url)\n",
    "    return {\"status\": \"success\", \"indexed_url\": results.get(\"indexed_url\", []), \"failed_url\": results.get(\"failed_url\", None)}\n",
    "\n",
    "@app.post(\"/api/v1/chat\")\n",
    "async def chat_endpoint(body: ChatRequest, x_api_key: str = Header(None)):\n",
    "    check_api_key(x_api_key)\n",
    "    try:\n",
    "        from __main__ import generate_answer\n",
    "    except Exception:\n",
    "        raise HTTPException(status_code=500, detail=\"RAG functions not found. Start server from notebook or ensure helper functions are accessible.\")\n",
    "    # Convert pydantic message objects to dicts\n",
    "    messages = [{\"role\": m.role, \"content\": m.content} for m in body.messages]\n",
    "    try:\n",
    "        resp = generate_answer(messages)\n",
    "        return {\"response\": [{\"answer\": {\"content\": resp[\"answer\"], \"role\": \"assistant\"}, \"citation\": resp.get(\"citations\", [])}]}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "'''\n",
    "\n",
    "# Write to app.py in the current directory\n",
    "with open(\"app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(APP_CODE)\n",
    "\n",
    "print(\"Wrote app.py to current directory. Run it with:\")\n",
    "print(\"uvicorn app:app --reload --port 8000\")\n",
    "print(\"Remember to set environment variable API_TOKEN to match X-API-KEY used by the client.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4e91289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote streamlit_app.py. Run it with:\n",
      "streamlit run streamlit_app.py --server.port 8501\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 — Write streamlit_app.py (UI) to disk\n",
    "STREAMLIT_CODE = r'''\n",
    "import streamlit as st\n",
    "import requests\n",
    "import json\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "st.set_page_config(page_title=\"RAG Website QA\", layout=\"centered\")\n",
    "\n",
    "st.title(\"RAG Website Q&A (with citations)\")\n",
    "\n",
    "# Inputs: API base + API key\n",
    "api_base = st.sidebar.text_input(\"API Base URL\", value=\"http://localhost:8000\")\n",
    "api_key = st.sidebar.text_input(\"X-API-KEY (API token)\", type=\"password\")\n",
    "\n",
    "st.sidebar.markdown(\"**Index a URL**\")\n",
    "url_to_index = st.sidebar.text_input(\"URL to index\")\n",
    "if st.sidebar.button(\"Index URL\"):\n",
    "    if not api_key:\n",
    "        st.sidebar.error(\"Set X-API-KEY in sidebar first\")\n",
    "    else:\n",
    "        try:\n",
    "            resp = requests.post(urljoin(api_base, \"/api/v1/index\"), json={\"url\":[url_to_index]}, headers={\"X-API-KEY\": api_key})\n",
    "            st.sidebar.write(resp.json())\n",
    "        except Exception as e:\n",
    "            st.sidebar.error(str(e))\n",
    "\n",
    "st.header(\"Ask a question\")\n",
    "messages = st.session_state.get(\"messages\", [{\"role\":\"assistant\", \"content\":\"Index some URLs first (use the sidebar) and then ask questions.\"}])\n",
    "user_input = st.text_input(\"Your question\", key=\"user_input\")\n",
    "\n",
    "if st.button(\"Ask\"):\n",
    "    if not api_key:\n",
    "        st.error(\"Set X-API-KEY in the sidebar.\")\n",
    "    else:\n",
    "        # prepare messages history (we send only the last user message suffice for RAG)\n",
    "        payload = {\"messages\": [{\"role\":\"user\",\"content\":user_input}]}\n",
    "        try:\n",
    "            resp = requests.post(urljoin(api_base, \"/api/v1/chat\"), json=payload, headers={\"X-API-KEY\": api_key})\n",
    "            data = resp.json()\n",
    "            # Response format: {\"response\":[{\"answer\":{\"content\": \"...\", \"role\":\"assistant\"}, \"citation\":[...]}]}\n",
    "            ans = data[\"response\"][0][\"answer\"][\"content\"]\n",
    "            citations = data[\"response\"][0].get(\"citation\", [])\n",
    "            st.markdown(\"### Answer\")\n",
    "            st.write(ans)\n",
    "            if citations:\n",
    "                st.markdown(\"### Citations\")\n",
    "                for c in citations:\n",
    "                    st.write(f\"- {c}\")\n",
    "        except Exception as e:\n",
    "            st.error(str(e))\n",
    "'''\n",
    "\n",
    "with open(\"streamlit_app.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(STREAMLIT_CODE)\n",
    "\n",
    "print(\"Wrote streamlit_app.py. Run it with:\")\n",
    "print(\"streamlit run streamlit_app.py --server.port 8501\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b6f2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
